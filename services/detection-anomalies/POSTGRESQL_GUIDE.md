# Guide PostgreSQL - Service Detection Anomalies

## Vue d'ensemble

Le service `detection-anomalies` utilise PostgreSQL pour journaliser toutes les anomalies d√©tect√©es, permettant l'historique, l'analyse et le reporting.

## ‚ö†Ô∏è Instructions obligatoires

### 1. Cr√©er la base de donn√©es PostgreSQL (OBLIGATOIRE)

**Tu dois ex√©cuter ces commandes SQL une seule fois** pour cr√©er la base de donn√©es et l'utilisateur :

```sql
CREATE DATABASE predictive_maintenance;
CREATE USER pmuser WITH PASSWORD 'pmpassword';
GRANT ALL PRIVILEGES ON DATABASE predictive_maintenance TO pmuser;
```

**Comment faire :**
- Connecte-toi √† PostgreSQL (via `psql` ou un client graphique)
- Ex√©cute les 3 commandes SQL ci-dessus
- C'est tout ! Le service cr√©era automatiquement les tables au d√©marrage

### 2. Configurer les variables d'environnement (OBLIGATOIRE)

**Tu dois configurer ces variables** dans ton fichier `.env` ou comme variables d'environnement :

```bash
DATABASE_HOST=localhost
DATABASE_PORT=5432
DATABASE_NAME=predictive_maintenance
DATABASE_USER=pmuser
DATABASE_PASSWORD=pmpassword
```

**C'est tout pour la configuration obligatoire !** Le service cr√©era automatiquement les tables et index au d√©marrage.

## üìã Instructions optionnelles (pour r√©f√©rence)

Les sections suivantes sont des **exemples et guides de r√©f√©rence** - tu n'as pas besoin de les ex√©cuter maintenant :

## Structure de la table (cr√©√©e automatiquement)

**‚ö†Ô∏è Tu n'as RIEN √† faire ici** - La table `anomaly_detections` est cr√©√©e automatiquement au d√©marrage du service :

```sql
CREATE TABLE anomaly_detections (
    id SERIAL PRIMARY KEY,
    asset_id VARCHAR(255) NOT NULL,
    sensor_id VARCHAR(255),
    timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
    final_score DECIMAL(5, 4) NOT NULL,
    is_anomaly BOOLEAN NOT NULL,
    criticality VARCHAR(20) NOT NULL,
    scores JSONB NOT NULL,
    features JSONB NOT NULL,
    metadata JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);
```

### Index

Les index suivants sont cr√©√©s automatiquement pour optimiser les requ√™tes :

- `idx_asset_id` - Sur `asset_id`
- `idx_sensor_id` - Sur `sensor_id`
- `idx_timestamp` - Sur `timestamp`
- `idx_is_anomaly` - Sur `is_anomaly`
- `idx_criticality` - Sur `criticality`
- `idx_asset_timestamp` - Composite sur `(asset_id, timestamp DESC)`
- `idx_sensor_timestamp` - Composite sur `(sensor_id, timestamp DESC)`

## Journalisation automatique (fonctionne automatiquement)

**‚ö†Ô∏è Tu n'as RIEN √† faire ici** - La journalisation fonctionne automatiquement :

- **Via l'API REST** : Les anomalies d√©tect√©es via `POST /api/v1/anomalies/detect` sont automatiquement journalis√©es si `is_anomaly=True`
- **Via le Worker Kafka** : Le worker Kafka journalise automatiquement toutes les anomalies d√©tect√©es en temps-r√©el

## üìñ Utilisation de l'API (exemples)

**Ces exemples montrent comment utiliser l'API** - tu peux les tester quand tu veux :

#### Toutes les anomalies

```bash
curl "http://localhost:8084/api/v1/anomalies/"
```

#### Filtrer par asset_id

```bash
curl "http://localhost:8084/api/v1/anomalies/?asset_id=ASSET001"
```

#### Filtrer par sensor_id

```bash
curl "http://localhost:8084/api/v1/anomalies/?sensor_id=SENSOR001"
```

#### Filtrer par dates

```bash
curl "http://localhost:8084/api/v1/anomalies/?start_date=2024-01-01T00:00:00Z&end_date=2024-01-31T23:59:59Z"
```

#### Filtrer par is_anomaly

```bash
# Seulement les anomalies
curl "http://localhost:8084/api/v1/anomalies/?is_anomaly=true"

# Seulement les normaux
curl "http://localhost:8084/api/v1/anomalies/?is_anomaly=false"
```

#### Filtrer par criticit√©

```bash
curl "http://localhost:8084/api/v1/anomalies/?criticality=high"
```

Valeurs possibles : `low`, `medium`, `high`, `critical`

#### Combinaison de filtres

```bash
curl "http://localhost:8084/api/v1/anomalies/?asset_id=ASSET001&criticality=high&is_anomaly=true&start_date=2024-01-01T00:00:00Z"
```

#### Pagination

```bash
# Premi√®re page (100 r√©sultats)
curl "http://localhost:8084/api/v1/anomalies/?limit=100&offset=0"

# Deuxi√®me page
curl "http://localhost:8084/api/v1/anomalies/?limit=100&offset=100"
```

### Format de r√©ponse

```json
{
  "anomalies": [
    {
      "id": 1,
      "asset_id": "ASSET001",
      "sensor_id": "SENSOR001",
      "timestamp": "2024-01-01T12:00:00Z",
      "final_score": 0.75,
      "is_anomaly": true,
      "criticality": "high",
      "scores": [
        {
          "model_name": "isolation_forest",
          "score": 0.8,
          "threshold": 0.5,
          "is_anomaly": true
        },
        {
          "model_name": "one_class_svm",
          "score": 0.7,
          "threshold": 0.5,
          "is_anomaly": true
        },
        {
          "model_name": "lstm_autoencoder",
          "score": 0.75,
          "threshold": 0.5,
          "is_anomaly": true
        }
      ],
      "features": {
        "rms": 10.5,
        "kurtosis": 2.3
      },
      "metadata": {
        "source": "kafka"
      },
      "created_at": "2024-01-01T12:00:00Z"
    }
  ],
  "total": 42,
  "limit": 100,
  "offset": 0,
  "filters": {
    "asset_id": "ASSET001",
    "sensor_id": null,
    "start_date": null,
    "end_date": null,
    "is_anomaly": null,
    "criticality": null
  }
}
```

## üíª Utilisation directe du service (exemple Python)

```python
from app.database.postgresql import PostgreSQLService
from app.models.anomaly_data import AnomalyDetectionResult

# Initialiser le service
db_service = PostgreSQLService()

# Ins√©rer une anomalie
anomaly_result = AnomalyDetectionResult(...)
anomaly_id = db_service.insert_anomaly(anomaly_result)

# R√©cup√©rer les anomalies
anomalies = db_service.get_anomalies(
    asset_id="ASSET001",
    start_date=datetime(2024, 1, 1),
    end_date=datetime(2024, 1, 31),
    is_anomaly=True,
    criticality="high",
    limit=100,
    offset=0
)

# Compter les anomalies
count = db_service.get_anomaly_count(
    asset_id="ASSET001",
    is_anomaly=True
)

# Fermer le service
db_service.close()
```

## üìä Requ√™tes SQL utiles (pour analyse - optionnel)

### Statistiques par asset

```sql
SELECT 
    asset_id,
    COUNT(*) as total_detections,
    SUM(CASE WHEN is_anomaly THEN 1 ELSE 0 END) as anomalies_count,
    AVG(final_score) as avg_score,
    MAX(final_score) as max_score
FROM anomaly_detections
GROUP BY asset_id
ORDER BY anomalies_count DESC;
```

### Statistiques par criticit√©

```sql
SELECT 
    criticality,
    COUNT(*) as count,
    AVG(final_score) as avg_score
FROM anomaly_detections
WHERE is_anomaly = true
GROUP BY criticality
ORDER BY count DESC;
```

### Anomalies r√©centes

```sql
SELECT 
    asset_id,
    sensor_id,
    timestamp,
    final_score,
    criticality
FROM anomaly_detections
WHERE is_anomaly = true
ORDER BY timestamp DESC
LIMIT 10;
```

### Tendances temporelles

```sql
SELECT 
    DATE_TRUNC('hour', timestamp) as hour,
    COUNT(*) as detections,
    SUM(CASE WHEN is_anomaly THEN 1 ELSE 0 END) as anomalies
FROM anomaly_detections
WHERE timestamp >= NOW() - INTERVAL '24 hours'
GROUP BY hour
ORDER BY hour;
```

## ‚öôÔ∏è Performance (information)

- **Pool de connexions** : ThreadedConnectionPool avec minconn=1, maxconn=10
- **Index optimis√©s** : Requ√™tes filtr√©es par asset_id, sensor_id, timestamp sont rapides
- **JSONB** : Colonnes scores, features, metadata utilisent JSONB pour des requ√™tes efficaces

## üîß D√©pannage (si probl√®me)

### Erreur de connexion

```
Erreur lors de l'initialisation du pool PostgreSQL: ...
```

**Solution** : V√©rifier que PostgreSQL est d√©marr√© et que les credentials sont corrects.

### Table non cr√©√©e

```
Erreur lors de la cr√©ation des tables: ...
```

**Solution** : V√©rifier les permissions de l'utilisateur PostgreSQL. Il doit avoir les droits CREATE TABLE.

### Requ√™tes lentes

**Solution** : V√©rifier que les index sont cr√©√©s :

```sql
SELECT indexname, indexdef 
FROM pg_indexes 
WHERE tablename = 'anomaly_detections';
```

## üõ†Ô∏è Maintenance (pour plus tard - optionnel)

### Nettoyage des anciennes donn√©es

```sql
-- Supprimer les anomalies de plus de 90 jours
DELETE FROM anomaly_detections
WHERE timestamp < NOW() - INTERVAL '90 days';
```

### Archivage

```sql
-- Cr√©er une table d'archive
CREATE TABLE anomaly_detections_archive (LIKE anomaly_detections INCLUDING ALL);

-- Archiver les donn√©es anciennes
INSERT INTO anomaly_detections_archive
SELECT * FROM anomaly_detections
WHERE timestamp < NOW() - INTERVAL '90 days';

-- Supprimer les donn√©es archiv√©es
DELETE FROM anomaly_detections
WHERE timestamp < NOW() - INTERVAL '90 days';
```

### Statistiques

```sql
-- Mettre √† jour les statistiques pour optimiser les requ√™tes
ANALYZE anomaly_detections;
```

